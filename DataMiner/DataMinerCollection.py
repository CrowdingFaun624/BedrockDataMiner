import json
from pathlib import Path
from typing import TYPE_CHECKING, Any, Iterator

import DataMiner.DataMiner as DataMiner
import Structure.DataPath as DataPath
import Structure.StructureBase as StructureBase
import Structure.StructureEnvironment as StructureEnvironment
import Structure.StructureTag as StructureTag
import Utilities.CustomJson as CustomJson
import Utilities.Exceptions as Exceptions
import Utilities.File as File
import Utilities.FileManager as FileManager
import Version.Version as Version

if TYPE_CHECKING:
    import DataMiner.DataMinerSettings as DataMinerSettings

NoneType = type(None)

class DataMinerCollection():

    def __init__(self, file_name:str, name:str, comparing_disabled:bool) -> None:
        self.name = name
        self.file_name = file_name
        self.comparing_disabled = comparing_disabled

        self.dataminer_settings:list["DataMinerSettings.DataMinerSettings"]|None = None
        self.structure:StructureBase.StructureBase|None = None

    def link_subcomponents(self, structure:StructureBase.StructureBase, dataminer_settings:list["DataMinerSettings.DataMinerSettings"]) -> None:
        self.structure = structure
        self.dataminer_settings = dataminer_settings

    def get_all_dataminer_settings(self) -> list["DataMinerSettings.DataMinerSettings"]:
        if self.dataminer_settings is None:
            raise Exceptions.AttributeNoneError("dataminer_settings", self)
        return self.dataminer_settings

    def get_structure(self) -> StructureBase.StructureBase:
        if self.structure is None:
            raise Exceptions.AttributeNoneError("structure", self)
        return self.structure

    def get_data_file(self, version:Version.Version, non_exist_ok:bool=False) -> Any:
        '''Opens the data file if it exists, and raises an error if it doesn't, or returns None if `non_exist_ok` is True'''
        data_path = FileManager.get_version_data_path(version.get_version_directory(), self.file_name)
        if not data_path.exists():
            if non_exist_ok:
                return None
            else:
                raise Exceptions.MissingDataFileError(self, self.file_name, version)
        with open(data_path, "rt") as f:
            return json.load(f, cls=CustomJson.decoder)

    def remove_data_file(self, version:Version.Version) -> None:
        data_path = FileManager.get_version_data_path(version.get_version_directory(), self.file_name)
        if data_path.exists():
            data_path.unlink()

    def has_tag(self, tag:StructureTag.StructureTag) -> bool:
        '''
        Returns True if the given tag could potentially be in this Version.
        :tag: The tag to test for.
        '''
        return self.get_structure().has_tag(tag)

    def get_tag_paths(self, version:Version.Version, tags:list[StructureTag.StructureTag], environment:StructureEnvironment.PrinterEnvironment, *, data:Any|None=None, normalized_data:Any|None=None) -> dict[StructureTag.StructureTag,list[DataPath.DataPath]]:
        dataminer = self.get_dataminer(version)
        if isinstance(dataminer, DataMiner.NullDataMiner):
            return {tag: [] for tag in tags}
        if data is None:
            data = dataminer.get_data_file()
        return self.get_structure().get_tag_paths(data, tags, environment, normalized_data=normalized_data)

    def compare(
            self,
            version1:Version.Version|None,
            version2:Version.Version,
            versions_between:list[Version.Version],
            environment:StructureEnvironment.StructureEnvironment,
            *,
            store:bool=True,
        ) -> str:
        '''Stores the comparison generated by this DataMinerCollection's Structure between two Versions, and returns the report.
        `data_cache` stores the output of `get_data_file`.'''
        if version1 is None:
            version2_data = self.get_data_file(version2)
            if hasattr(version2_data, "__copy_empty__"):
                version1_data = version2_data.__copy_empty__()
            else:
                version1_data = type(version2_data)() # create new empty object.
        else:
            version1_data = self.get_data_file(version1)
            version2_data = self.get_data_file(version2)
        report, had_changes = self.get_structure().comparison_report(version1_data, version2_data, version1, version2, versions_between, environment)
        if store and had_changes:
            self.get_structure().store(report, self.name)
        return report

    def check_types(self, version:Version.Version, environment:StructureEnvironment.PrinterEnvironment) -> None:
        if not self.supports_version(version):
            return
        data = self.get_data_file(version)
        structure = self.get_structure()
        normalized_data = structure.normalize(data, environment)
        structure.check_types(normalized_data, environment.structure_environment, (version,))

    def __hash__(self) -> int:
        return hash(self.name)

    def __repr__(self) -> str:
        return "<DataMinerCollection %s>" % self.name

    def clear_caches(self) -> None:
        '''Clears all caches of this DataMinerCollection's Structure.'''
        self.get_structure().clear_caches()

    def clear_some_caches(self) -> None:
        '''Clears items from caches of this DataMinerCollection's Structure and all of its children that are too old.'''
        self.get_structure().clear_some_caches()

    def get_dataminer_settings(self, version:Version.Version) -> "DataMinerSettings.DataMinerSettings":
        '''Returns a DataMinerSettings such that `version` is in the dataminer's VersionRange'''
        for dataminer_setting in self.get_all_dataminer_settings():
            if version in dataminer_setting.get_version_range():
                return dataminer_setting
        else: raise Exceptions.InvalidStateError("Version matches no DataMinerSettings!")

    def get_dataminer(self, version:Version.Version) -> DataMiner.DataMiner:
        '''Returns a DataMiner such that `version` is in the dataminer's VersionRange.'''
        dataminer_settings = self.get_dataminer_settings(version)
        return dataminer_settings.get_dataminer_class()(version, dataminer_settings)

    def supports_version(self, version:Version.Version) -> bool:
        dataminer_settings = self.get_dataminer_settings(version)
        if dataminer_settings.dataminer_class is DataMiner.NullDataMiner:
            return False
        version_files = set(version_file.get_version_file_type() for version_file in version.get_version_files() if version_file.has_accessors())
        return all(
            required_version_file_type in version_files # cannot only datamine it if required files are accessible
            for required_version_file_type in dataminer_settings.get_version_file_types()
        ) and all(
            dependency.supports_version(version)
            for dependency in dataminer_settings.get_dependencies()
        )

    def get_data_file_path(self, version:Version.Version) -> Path:
        return FileManager.get_version_data_path(version.get_version_directory(), self.file_name)

    def get_referenced_files(self, version:Version.Version, structure_tags:dict[str,StructureTag.StructureTag]) -> Iterator[int]:
        structure_environment = StructureEnvironment.StructureEnvironment(StructureEnvironment.EnvironmentType.garbage_collection)
        data_file = self.get_data_file(version, non_exist_ok=True)
        if data_file is None: return
        yield from File.recursive_examine_data_for_files(data_file) # this is necessary just in case there's a file that's ignored by the structure.
        structure = self.get_structure()
        file_tags = [structure_tag for structure_tag in structure_tags.values() if structure_tag.is_file]
        if structure.children_has_garbage_collection or structure.has_tags(file_tags):
            environment = StructureEnvironment.PrinterEnvironment(structure_environment, None, version, 0)
            normalized_data = structure.normalize(data_file, environment)
            yield from structure.get_referenced_files(data_file, environment, normalized_data=normalized_data) # this is necessary just in case files appear only after normalization
            for file_tag, paths in self.get_tag_paths(version, file_tags, environment, normalized_data=normalized_data).items(): # this is necessary just in case files are referenced only by a hash that isn't used.
                for data_path in paths:
                    match data_path.embedded_data:
                        case str():
                            yield File.hash_str_to_int(data_path.embedded_data)
                        case int():
                            yield data_path.embedded_data
                        case _:
                            raise Exceptions.InvalidFileHashType(version, file_tag, data_path)
