import json
from pathlib import Path
from typing import Any, Iterator

import Dataminer.DataminerEnvironment as DataminerEnvironment
import Domain.Domain as Domain
import Structure.DataPath as DataPath
import Structure.StructureBase as StructureBase
import Structure.StructureEnvironment as StructureEnvironment
import Structure.StructureTag as StructureTag
import Utilities.Exceptions as Exceptions
import Utilities.File as File
import Version.Version as Version


class AbstractDataminerCollection():

    def __init__(self, file_name:str, name:str, domain:"Domain.Domain", comparing_disabled:bool) -> None:
        self.name = name
        self.file_name = file_name
        self.domain = domain
        self.comparing_disabled = comparing_disabled

    def link_subcomponents(self, structure:StructureBase.StructureBase) -> None:
        self.structure:StructureBase.StructureBase = structure

    def datamine(self, version:Version.Version, environment:DataminerEnvironment.DataminerEnvironment) -> Any: ...

    def store(self, version:Version.Version, environment:DataminerEnvironment.DataminerEnvironment) -> Any:
        '''Makes the DataminerCollection get the file. Returns the output and stores it in a file.'''
        data = self.datamine(version, environment)
        if data is None:
            raise Exceptions.DataminerNullReturnError(self)
        version.data_directory.mkdir(exist_ok=True)

        data_file_path = self.get_data_file_path(version)
        parents_to_create:list[Path] = []
        for parent in data_file_path.parents:
            if parent == version.data_directory or parent.exists(): break
            parents_to_create.append(parent)
        else:
            raise Exceptions.InvalidFileLocationError(data_file_path, version.data_directory)
        parents_to_create.reverse()
        for parent in parents_to_create:
            parent.mkdir()

        with open(data_file_path, "wt") as f:
            json.dump(data, f, separators=(",", ":"), cls=self.domain.json_encoder)

        if self.structure is not None:
            normalized_data = self.structure.normalize(data, environment.get_printer_environment(version))
            self.structure.check_types(normalized_data, environment.structure_environment, (version,))

        return self.get_data_file(version) # since the normalizing immediately before may modify it.

    def get_dependencies(self, version:Version.Version) -> list["AbstractDataminerCollection"]: ...

    def get_data_file(self, version:Version.Version, non_exist_ok:bool=False) -> Any:
        '''Opens the data file if it exists, and raises an error if it doesn't, or returns None if `non_exist_ok` is True'''
        data_path = version.data_directory.joinpath(self.file_name)
        if not data_path.exists():
            if non_exist_ok:
                return None
            else:
                raise Exceptions.MissingDataFileError(self, self.file_name, version)
        with open(data_path, "rt") as f:
            return json.load(f, cls=self.domain.json_decoder)

    def remove_data_file(self, version:Version.Version) -> None:
        data_path = version.data_directory.joinpath(self.file_name)
        if data_path.exists():
            data_path.unlink()
        for parent in data_path.parents:
            if parent == version.data_directory:
                break
            if parent.exists():
                try:
                    parent.rmdir()
                except OSError:
                    break

    def has_tag(self, tag:StructureTag.StructureTag) -> bool:
        '''
        Returns True if the given tag could potentially be in this Version.
        :tag: The tag to test for.
        '''
        return self.structure.has_tag(tag)

    def get_tag_paths(self, version:Version.Version, tags:list[StructureTag.StructureTag], environment:StructureEnvironment.PrinterEnvironment, *, data:Any|None=None, normalized_data:Any|None=None) -> dict[StructureTag.StructureTag,list[DataPath.DataPath]]:
        if not self.supports_version(version):
            return {tag: [] for tag in tags}
        if data is None:
            data = self.get_data_file(version)
        return self.structure.get_tag_paths(data, tags, environment, normalized_data=normalized_data)

    def compare(
            self,
            version1:Version.Version|None,
            version2:Version.Version,
            versions_between:list[Version.Version],
            environment:StructureEnvironment.StructureEnvironment,
            *,
            store:bool=True,
        ) -> str:
        '''Stores the comparison generated by this DataminerCollection's Structure between two Versions, and returns the report.
        `data_cache` stores the output of `get_data_file`.'''
        if version1 is None:
            version2_data = self.get_data_file(version2)
            if hasattr(version2_data, "__copy_empty__"):
                version1_data = version2_data.__copy_empty__()
            else:
                version1_data = type(version2_data)() # create new empty object.
        else:
            version1_data = self.get_data_file(version1)
            version2_data = self.get_data_file(version2)
        report, had_changes = self.structure.comparison_report(version1_data, version2_data, version1, version2, versions_between, environment)
        if store and had_changes:
            self.structure.store(report, self.name)
        return report

    def check_types(self, version:Version.Version, environment:StructureEnvironment.PrinterEnvironment) -> None:
        if not self.supports_version(version):
            return
        data = self.get_data_file(version)
        structure = self.structure
        normalized_data = structure.normalize(data, environment)
        structure.check_types(normalized_data, environment.structure_environment, (version,))

    def __hash__(self) -> int:
        return hash(self.name)

    def __repr__(self) -> str:
        return f"<{self.__class__.__name__} {self.name}>"

    def clear_caches(self) -> None:
        '''Clears all caches of this DataminerCollection's Structure.'''
        self.structure.clear_caches()

    def clear_some_caches(self) -> None:
        '''Clears items from caches of this DataminerCollection's Structure and all of its children that are too old.'''
        self.structure.clear_some_caches()

    def supports_version(self, version:Version.Version) -> bool:
        return True

    def get_data_file_path(self, version:Version.Version) -> Path:
        return version.data_directory.joinpath(self.file_name)

    def get_referenced_files(self, version:Version.Version, structure_tags:dict[str,StructureTag.StructureTag]) -> Iterator[int]:
        structure_environment = StructureEnvironment.StructureEnvironment(StructureEnvironment.EnvironmentType.garbage_collection, self.domain)
        data_file = self.get_data_file(version, non_exist_ok=True)
        if data_file is None: return
        yield from File.recursive_examine_data_for_files(data_file) # this is necessary just in case there's a file that's ignored by the structure.
        structure = self.structure
        file_tags = [structure_tag for structure_tag in structure_tags.values() if structure_tag.is_file]
        if structure.children_has_garbage_collection or structure.has_tags(file_tags):
            environment = StructureEnvironment.PrinterEnvironment(structure_environment, None, version, 0)
            normalized_data = structure.normalize(data_file, environment)
            yield from structure.get_referenced_files(data_file, environment, normalized_data=normalized_data) # this is necessary just in case files appear only after normalization
            for file_tag, paths in self.get_tag_paths(version, file_tags, environment, normalized_data=normalized_data).items(): # this is necessary just in case files are referenced only by a hash that isn't used.
                for data_path in paths:
                    match data_path.embedded_data:
                        case str():
                            yield File.hash_str_to_int(data_path.embedded_data)
                        case int():
                            yield data_path.embedded_data
                        case _:
                            raise Exceptions.InvalidFileHashType(version, file_tag, data_path)
